{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e46381-b03b-4e96-ba8c-5971be51db7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 2097152 bytes (33681923 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/yasserhessein/dataset-alzheimer?dataset_version_number=1 (2097152/35779075) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34.1M/34.1M [02:47<00:00, 201kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ghost/.cache/kagglehub/datasets/yasserhessein/dataset-alzheimer/versions/1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset\n",
    "import kagglehub\n",
    "kagglehub.dataset_download(\"yasserhessein/dataset-alzheimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6004b4d0-cecc-4bf8-94f8-8c0a8253b0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load dataset\n",
    "train_dir = \"./dataset/train\"\n",
    "test_dir = \"./dataset/test\"\n",
    "\n",
    "categories = [\"MildDemented\", \"ModerateDemented\", \"NonDemented\", \"VeryMildDemented\"]\n",
    "\n",
    "# Define image size and batch size\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data Preprocessing Function\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(folder, category)\n",
    "        label = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_path = os.path.join(path, img)\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, IMG_SIZE)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load training and testing images\n",
    "X_train, y_train = load_images_from_folder(train_dir)\n",
    "X_test, y_test = load_images_from_folder(test_dir)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = X_train.reshape(-1, 128, 128, 1)\n",
    "X_test = X_test.reshape(-1, 128, 128, 1)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=len(categories))\n",
    "\n",
    "# Build Classification Model\n",
    "def build_classification_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(categories), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "classification_model = build_classification_model()\n",
    "\n",
    "# Train the model\n",
    "history = classification_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Build U-Net Segmentation Model\n",
    "def unet_model(input_size=(128, 128, 1)):\n",
    "    inputs = keras.Input(input_size)\n",
    "    \n",
    "    # Encoding Path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    \n",
    "    # Decoding Path\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u1 = layers.concatenate([u1, c2])\n",
    "    c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u2 = layers.UpSampling2D((2, 2))(c4)\n",
    "    u2 = layers.concatenate([u2, c1])\n",
    "    c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "segmentation_model = unet_model()\n",
    "\n",
    "# Print Model Summary\n",
    "segmentation_model.summary()\n",
    "\n",
    "# Predict and visualize segmentation results\n",
    "def visualize_segmentation(sample_images, model):\n",
    "    fig, axes = plt.subplots(len(sample_images), 3, figsize=(10, len(sample_images) * 3))\n",
    "    for i, img in enumerate(sample_images):\n",
    "        img = img / 255.0  # Normalize\n",
    "        img_input = img.reshape(1, 128, 128, 1)\n",
    "        prediction = model.predict(img_input)[0].reshape(128, 128)\n",
    "        \n",
    "        axes[i, 0].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Original Image\")\n",
    "        axes[i, 1].imshow(prediction, cmap='jet')\n",
    "        axes[i, 1].set_title(\"Segmentation Output\")\n",
    "        axes[i, 2].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i, 2].imshow(prediction, cmap='jet', alpha=0.5)\n",
    "        axes[i, 2].set_title(\"Overlay\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select a few test images for visualization\n",
    "sample_test_images = X_test[:5]\n",
    "visualize_segmentation(sample_test_images, segmentation_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
